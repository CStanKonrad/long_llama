{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LongLLaMA: Focused Transformer Training for Context Scaling\n",
        "**LongLLaMA is a large language model capable of handling long contexts of 256k tokens or even more**.\n",
        "\n",
        "It is built upon the foundation of [OpenLLaMA](https://github.com/openlm-research/open_llama) and fine-tuned using the [Focused Transformer (FoT)](https://arxiv.org/abs/2307.03170) method.\n",
        "\n",
        "This notebook is a demo of [LongLLaMA-Instruct-3Bv1.1](https://huggingface.co/syzymon/long_llama_3b_instruct), a [LongLLaMA-3Bv1.1](https://huggingface.co/syzymon/long_llama_3b_v1_1) fine-tuned using [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) and [ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed) datasets. Note that LongLLaMA-Instruct 3B is licensed differently due to the use of responses from GPT-4/GPT-3.5.\n",
        "Similarly to the [LongLLaMA 3B](https://huggingface.co/syzymon/long_llama_3b), the model weights can serve as the drop-in replacement of LLaMA in existing implementations (for short context up to 2048 tokens).\n",
        "\n",
        "This notebook is a research preview of [LongLLaMA-Instruct-3Bv1.1](https://huggingface.co/syzymon/long_llama_3b_instruct).\n",
        "For more, see the [FoT paper](https://arxiv.org/abs/2307.03170) and [GitHub repository](https://github.com/CStanKonrad/long_llama)."
      ],
      "metadata": {
        "id": "pqHTEvk1yO9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial steps"
      ],
      "metadata": {
        "id": "jSRPvaWbzDc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHZPu2syyMFm"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers==4.30.0  sentencepiece accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import LlamaTokenizer, AutoModelForCausalLM, TextStreamer, PreTrainedModel, PreTrainedTokenizer\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "PTpgfIAfzG6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = (\n",
        "    \"syzymon/long_llama_3b_instruct\"\n",
        ")\n",
        "TOKENIZER_PATH = MODEL_PATH\n",
        "# to fit into colab GPU we will use reduced precision\n",
        "TORCH_DTYPE = torch.bfloat16\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "LDmETzwyzJd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(TOKENIZER_PATH)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=TORCH_DTYPE,\n",
        "    device_map=device,\n",
        "    trust_remote_code=True,\n",
        "    # mem_attention_grouping is used\n",
        "    # to trade speed for memory usage\n",
        "    # for details, see the section Additional configuration\n",
        "    mem_attention_grouping=(1, 2048),\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Q-NGIVKbzTWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The demo"
      ],
      "metadata": {
        "id": "UJAmrLV1zVWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization and question answering\n",
        "We used the [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset to instruction tune the model.\n",
        "Here we show the ability of the model to answer questions about long documents.\n",
        "Note that as the model used in the demo has only 3B parameters, it can have trouble understanding complex documents."
      ],
      "metadata": {
        "id": "sXGSRa4AzXq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "def get_paper(url: str, main_file: str):\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        archive_dir = os.path.join(tmp_dir, \"_archive.tar.gz\")\n",
        "        urllib.request.urlretrieve(url, archive_dir)\n",
        "\n",
        "        shutil.unpack_archive(archive_dir, tmp_dir)\n",
        "\n",
        "        with open(os.path.join(tmp_dir, main_file), \"r\") as f:\n",
        "            data = f.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def load_to_memory(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, text: str):\n",
        "    tokenized_data = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_data.input_ids\n",
        "    input_ids = input_ids.to(model.device)\n",
        "    torch.manual_seed(0)\n",
        "    output = model(input_ids=input_ids)\n",
        "    memory = output.past_key_values\n",
        "    return memory\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with_memory(\n",
        "    model: PreTrainedModel, tokenizer: PreTrainedTokenizer, memory, prompt: str, temperature=0.6\n",
        "):\n",
        "    tokenized_data = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_data.input_ids\n",
        "    input_ids = input_ids.to(model.device)\n",
        "\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
        "\n",
        "    new_memory = memory\n",
        "\n",
        "    stop = False\n",
        "    while not stop:\n",
        "        output = model(input_ids, past_key_values=new_memory)\n",
        "        new_memory = output.past_key_values\n",
        "        assert len(output.logits.shape) == 3\n",
        "        assert output.logits.shape[0] == 1\n",
        "        last_logit = output.logits[[0], [-1], :]\n",
        "        dist = torch.distributions.Categorical(logits=last_logit / temperature)\n",
        "        next_token = dist.sample()\n",
        "        if next_token[0] == tokenizer.eos_token_id:\n",
        "            streamer.put(next_token[None, :])\n",
        "            streamer.end()\n",
        "            stop = True\n",
        "        else:\n",
        "            input_ids = next_token[None, :]\n",
        "            streamer.put(input_ids)\n",
        "\n",
        "\n",
        "PROMPT_PREFIX = \"You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can.\\n\"\n",
        "\n",
        "\n",
        "def construct_question_prompt(question: str):\n",
        "    prompt = f\"\\nAnswer the following question breifly using information from the text above.\\nQuestion: {question}\\nAnswer: \"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def ask_model(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prompt: str, memory, seed=0):\n",
        "    tokenized_data = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_data.input_ids\n",
        "    input_ids = input_ids.to(model.device)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    generate_with_memory(model, tokenizer, memory, prompt)"
      ],
      "metadata": {
        "id": "IF6Cv37mzgJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fot_paper = get_paper(url=\"https://arxiv.org/e-print/2307.03170\", main_file=\"neurips_2023.tex\")\n",
        "fot_memory = load_to_memory(model, tokenizer, PROMPT_PREFIX + fot_paper)"
      ],
      "metadata": {
        "id": "S2Y97wGUzmtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = construct_question_prompt(\"What is the paper above about? Summarize it briefly.\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ],
      "metadata": {
        "id": "vF--q7Y9zoWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = construct_question_prompt(\"What method is introduced in the paper?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ],
      "metadata": {
        "id": "Dd46qCjq0M4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = construct_question_prompt(\"How is the 3B model called by the authors?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ],
      "metadata": {
        "id": "hNcRJhsN0P99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = construct_question_prompt(\"Name at least one author of the presented paper.\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ],
      "metadata": {
        "id": "RxnqzZlI0Skg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = construct_question_prompt(\"What is the distraction issue?\")\n",
        "ask_model(model, tokenizer, prompt, fot_memory)"
      ],
      "metadata": {
        "id": "wu4UKMS70TS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat\n",
        "We have also used [ShareGPT-Processed](https://huggingface.co/datasets/zetavg/ShareGPT-Processed) dataset to enhance the model conversation abilities. The chat prompt was inspired by [LongChat](https://github.com/DachengLi1/LongChat)."
      ],
      "metadata": {
        "id": "Ize5Yx3d0WvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatOutputBuffer:\n",
        "    \"\"\"\n",
        "    For providing online output that\n",
        "    is truncated after generating specified (stop_text)\n",
        "    sequence of characters\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stop_text: List[str], tokenizer: PreTrainedModel):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
        "        self.max_stop_seq = 0\n",
        "        self.stop_seq = []\n",
        "        for st in stop_text:\n",
        "            self.stop_seq.append(st)\n",
        "            self.max_stop_seq = max(self.max_stop_seq, len(st))\n",
        "\n",
        "        self.output_buffer = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "    def reset_output_buffer(self):\n",
        "        self.output_buffer = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "    def advance_output(self):\n",
        "        beg = 0\n",
        "        end = len(self.output_buffer) - self.max_stop_seq\n",
        "\n",
        "        if end > beg:\n",
        "            output = self.output_buffer[beg:end]\n",
        "            self.streamer.put(output)\n",
        "            self.output_buffer = self.output_buffer[end:]\n",
        "\n",
        "    def flush_buffer(self):\n",
        "        if len(self.output_buffer) > 0:\n",
        "            self.streamer.put(self.output_buffer)\n",
        "            self.output_buffer = self.output_buffer[len(self.output_buffer) :]\n",
        "        self.streamer.end()\n",
        "\n",
        "    def generation_too_long(self, text: str) -> int:\n",
        "        end_requests = 0\n",
        "        for st in self.stop_seq:\n",
        "            if text.endswith(st):\n",
        "                end_requests += 1\n",
        "        return end_requests\n",
        "\n",
        "    def update_buffer(self, next_tok: int) -> bool:\n",
        "        assert isinstance(next_tok, int)\n",
        "\n",
        "        array_next_tok = np.array([next_tok], dtype=np.int64)\n",
        "        self.output_buffer = np.concatenate([self.output_buffer, array_next_tok], axis=0)\n",
        "\n",
        "        suffix = self.output_buffer[-self.max_stop_seq :]\n",
        "        decoded = self.tokenizer.decode(suffix)\n",
        "        end_requests = self.generation_too_long(decoded)\n",
        "        if end_requests > 0:\n",
        "            decoded = self.tokenizer.decode(suffix[1:])\n",
        "            while self.generation_too_long(decoded) == end_requests:\n",
        "                suffix = suffix[1:]\n",
        "                decoded = self.tokenizer.decode(suffix[1:])\n",
        "\n",
        "            left_intact = len(self.output_buffer) - len(suffix)\n",
        "\n",
        "            self.output_buffer = self.output_buffer[:left_intact]\n",
        "            self.flush_buffer()\n",
        "            return True\n",
        "\n",
        "        self.advance_output()\n",
        "        return False\n",
        "\n",
        "\n",
        "class SimpleChatBot:\n",
        "    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.prompt = \"A chat between a user (denoted as USER:) and an artificial intelligence assistant (denoted as ASSISTANT:). The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\\n\"\n",
        "        self.model_name = \"\\nASSISTANT: \"\n",
        "        self.user_name = \"\\nUSER: \"\n",
        "        self.past_key_values = None\n",
        "\n",
        "        self.t = 0.8\n",
        "        self.output_buffer = ChatOutputBuffer(\n",
        "            [self.model_name.strip(), self.user_name.strip(), self.tokenizer.eos_token], self.tokenizer\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ask(self, text: str):\n",
        "        base_prompt = self.prompt if self.past_key_values is None else \"\"\n",
        "        prompt = base_prompt + self.user_name + text + self.model_name\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "        if self.past_key_values is None:\n",
        "            input_ids = torch.concatenate(\n",
        "                [torch.tensor([self.tokenizer.bos_token_id], dtype=torch.long).reshape(1, 1), input_ids], dim=-1\n",
        "            )\n",
        "\n",
        "        self.output_buffer.reset_output_buffer()\n",
        "        output_text = self.model_name\n",
        "        output_ids = self.tokenizer.encode(\n",
        "            output_text, return_tensors=\"pt\", add_special_tokens=self.past_key_values is None\n",
        "        )\n",
        "        self.output_buffer.streamer.put(output_ids)\n",
        "\n",
        "        is_writing = True\n",
        "\n",
        "        while is_writing:\n",
        "            input_ids = input_ids.to(model.device)\n",
        "            output = self.model(input_ids=input_ids, past_key_values=self.past_key_values)\n",
        "\n",
        "            logits = output.logits\n",
        "            assert len(logits.shape) == 3\n",
        "            assert logits.shape[0] == 1\n",
        "            last_logit = logits[[0], [-1], :]\n",
        "\n",
        "            dist = torch.distributions.Categorical(logits=last_logit / self.t)\n",
        "            next_token = dist.sample()\n",
        "            # Note that parts of cut out text may remain in model memory\n",
        "            # this is implemented in this way for performance reasons\n",
        "            past_key_values = output.past_key_values\n",
        "            assert len(next_token.shape) == 1\n",
        "            should_stop = self.output_buffer.update_buffer(next_token[0].cpu().item())\n",
        "            if should_stop:\n",
        "                is_writing = False\n",
        "            else:\n",
        "                input_ids = next_token[None, :]\n",
        "                self.past_key_values = past_key_values"
      ],
      "metadata": {
        "id": "2SvOL0iw0YqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to try the chat yourself:"
      ],
      "metadata": {
        "id": "MekVis480zfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = SimpleChatBot(model=model, tokenizer=tokenizer)\n",
        "while True:\n",
        "    user_text = input(\"USER: \")\n",
        "    chatbot.ask(user_text)"
      ],
      "metadata": {
        "id": "uwJALuqc00t2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
